<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[feature_preprocess]]></title>
    <url>%2F2019%2F01%2F20%2Ffeature-preprocess%2F</url>
    <content type="text"><![CDATA[Numeric Feature对于线性模型需要Feature scaling，不同的scale会对梯度下降有很大的影响，scale大的feature的微小变动对结果影响很大。 Scaling MinMaxScaler：不改变分布 StandardScaler Outliers使用可视化的方法，发现outliers。 Feature clipping: 使用99%的分位数作为最大的。 Rank：使用排序代替原始值，如果无法使用可视化发现outliers 使数据变紧凑 np.log(1+x), np.sqrt(1+x) Categorical and ordinal featurescategorical feature没有序，例如：性别。ordinal feature有序，例如：仓位，年级。 对于Tree，使用Label encoding或者Frequency encoding。对于其他模型，使用One-hot necoding。不同的categorical feature或者ordinal feature之间可以做交互。例如：性别和仓位，变成1male, 2male, 1female, 2female。这样的做法对于线性模型有很大帮助。 Datetime and coordinates features对于Datetime feature可以转换为，day of week, week of year, season of year；可以转换为距离某一个特定日期的距离，例如距离周末的距离；是否是节假日； 对于Coordinates feature可以转化为距离某一个点，可以是train set中的，也可以是额外的(地标，城市中心)的距离，转换为格子内的统计信息。 缺失值缺失值的类型有：NA，空字符串，-1，很大的数字，-99999，999，99。通过可视化的方式，可以找出缺失值。 可以使用mean或者median值填充，也可以使用拟合的方式填充。增加一列标记是否缺失的二值变量。 不要在产生新的feature之前对缺失值做填充，否则会影响产生的feature。 标准化在机器学习算法的目标函数(例如SVM的RBF内核或线性模型的l1和l2正则化)，许多学习算法中目标函数的基础都是假设所有的特征都是零均值并且具有同一阶数上的方差。如果某个特征的方差比其他特征大几个数量级，那么它就会在学习算法中占据主导位置，导致学习器并不能像我们说期望的那样，从其他特征中学习。 标准化能够加速梯度下降，如下图：左边蓝色的图片，由于$x_2$的范围比较大，因此$w_2$的微小变化损失函数就有很大的变化。反之$w_1$要变化很多损失函数才有明显变化。从梯度下降的公式来说，$w_1$的变化取决于$x_1$，因此梯度很小，那么需要很久才能下降到最低点。右边的图片做了标准化，能够很快的下降到最低点。 其他变换QuantileTransformer把数据变成对应的分为数，例如原本处于0.25分位数的数字是6.8，变化之后就是0.25。这个变换是一个非线性变换。 归一化，是 缩放单个样本以具有单位范数的过程。]]></content>
  </entry>
  <entry>
    <title><![CDATA[张江895孵化器访问]]></title>
    <url>%2F2019%2F01%2F16%2F%E5%BC%A0%E6%B1%9F895%E5%AD%B5%E5%8C%96%E5%99%A8%E8%AE%BF%E9%97%AE%2F</url>
    <content type="text"><![CDATA[由于寒假社会生活实践的原因，今天我拜访了位于张江高科的895医疗健康孵化器。这家公司成立于2013年，于2017年12月正式开始运营孵化器，大部分的公司处在A轮之前，产品处于生产出来之前。目前有8家孵化企业，追求小而精的团队。其中设计AI的有3家，大部分是医疗器械和智慧养老。这家孵化器属于民营公司，主要通过已有的创业经验来帮助创业团队，帮助走过创业的初期。对比学校的孵化器所做的产品更加深入，对比政府的孵化器，能够在应用和技术的细节上为创业团队提供帮助，而不只是政策引导的帮助(政府的孵化器相当于是老教授，民营孵化器相当于是高年级学长)。管理团队来自各个创业团队的主要成员。895医疗健康孵化器对于挑选入住孵化器的团队主要看3个方面，(1)技术创新性，临床需求；(2)公司的成熟度是否和孵化器匹配；(3)和已经入住的团队是否匹配。 这是我第一次了解什么是孵化器，孵化器有什么作用，以及人工智能在医疗领域的应用现状。 一般来讲，创业开始都是有了一个不错的idea或者是有了一项创新的技术。但是创立一个公司却不单单是这些。创始人要考虑出了技术之外的很多方面，例如：财务，法务，客户等等问题。一个没有经验的创业者，会被这些问题阻碍。因此针对出了技术之外的繁杂问题，孵化器主要从以下三个方面提供了帮助，(1)软硬件设施；(2)行业资源；(3)生态圈。 软硬件设施。首先是孵化器为创业团队提供了办公场所，但这是最不重要的一点。如果只是办公场所，孵化器和二房东就没有区别了。例如：一个团队做出来算法的创新，需要做一款APP来推向市场。传统的方式是自己聘用做APP的人，或者外包出去。前者由于开发不是重点，如果没有足够的资金，那么难以保证质量，而且一时间也难以组建一个合适的开发团队。后者外包的开发的团队一般对于创业的领域了解程度不够，在对接需求上会花费大量的经历，也会影响产品的质量(这一点我深有体会，在2018年11月上线一个产品时，由于开发团队没有金融的基础，每次新来一个业务人员，我需要把整个背景讲一遍，处理问题的细节时也经常出错)。孵化器里面的企业类型相同或者相似，直接配以开发团队，轮流为各个创业团队提供开发服务，既为开发团队解决了不必要的问题，又深入了业务，减少了大量的错误，节省了成本。除此之外，一个产品开发出来，后期的跟踪很重要。但是创业团队不一定有专业的专门的人负责这件事情。例如：监测老人身体状况的手环，孵化器配备的人员会定期回访手环使用情况。检测到老人摔倒，到底是真的摔倒，还是老人在做什么事情。医疗数据有数据量小，但是要求准确性高的特点。通过后期跟踪，能够及时的得到数据反馈，调整模型。第三，平台能够让产品尽快实践，得到实际的反馈，产品操作是否便捷，培训是否简单。这些决定了最终是否有人买账。以上的软硬件设施或者团队，在创业初期很难建设完全。但是这些模块缺一不可，孵化器就成了一个很好的支持。 行业资源。医疗器械的成熟慢，一个产品要5-8年的周期，过程中要设计方方面面。因此把一个想法或者一个算法变成产品，行业资源是不可缺少的。在医疗行业，国家有非常严格的监管。对于每一个产品如何验证其安全性，如何设计临床实验，如何保证生产中的质量。这些大坑足以把绝大部分创业者卡死，毕竟医疗行业产品要保证最差的时候也要有很高的准确率。一次错误也不能犯。孵化器在这些方面有丰富的资源，能够及时的为创业团队提供这些服务。 生态圈。孵化器中同时存在相同类型的多家公司，是一个很好的生态圈，不同公司之间往往会擦出不可思议的火花。例如：一个做医疗穿戴的团队和智慧社区养老的团队合作，同时解决了两个团队的问题。在一个大的平台上能够拓宽思路和产品的销路。 总体来说，医疗行业和互联网行业还是有很大的区别。医疗行业的成熟速度慢，很保守，产品要有严格的安全性。尽管目前有很多论文，但是产品的下限还难以保证，因此人工智能在医疗领域的应用还有很大的发展空间。 待续。。。]]></content>
      <categories>
        <category>生活游记</category>
      </categories>
      <tags>
        <tag>生活游记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GANs]]></title>
    <url>%2F2019%2F01%2F14%2FGANs%2F</url>
    <content type="text"><![CDATA[对抗生成网络笔记这篇文章用来记录各种GAN的原理和应用。 EBGAN(ENERGY-BASED GENERATIVE ADVERSARIAL NETWORKS)作者：Junbo Zhao, Michael Mathieu and Yann LeCun发表：ICLR 2017]]></content>
  </entry>
  <entry>
    <title><![CDATA[集成学习]]></title>
    <url>%2F2019%2F01%2F05%2F%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[随机森林]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>Ensemble Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树]]></title>
    <url>%2F2019%2F01%2F05%2F%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[决策树优点 容易解释，可以可视化。 对数据预处理的要求低，不需要做数据归一化，允许数据有缺失。 可以处理连续型变量和离散型变量。 决策树的缺点 容易过拟合，可以通过剪枝，设置每个节点最小样本数量，树的最大深度来避免。 容易收到干扰，某些数据的微小变化可能带来树的巨大变化。其实就是方差的问题，减小方差可以通过ensemble的方式缓解。 无法保证找到最优的树，找到最优的树是一个NPC问题。因此，每个节点分裂时，使用贪心算法。只能得到局部最优解。可以通过ensemble的方式缓解，训练每棵树时对数据和特征做采样。 There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems. 对于数据不平衡问题，树可能有很大的偏差，需要先解决数据不平衡问题。 时间复杂度分析使用技巧 数据量和特征数量要匹配，数据很少，但是特征很多很容易过拟合。 提前使用PCA，ICA或者Feature Selection降维可以让树学的更好。 事先解决数据不平衡问题。例如通过采样方式，或者通过normalizing the sum of the sample weights for each class的方式实现。 不同类型的树ID3ID3树每个节点有多个分支，根据信息增益来作为分裂标准。在树生成之后使用剪枝来防止过拟合。 缺点：只能使用离散特征，只能针对分类问题。 信息熵，表示数据的不确定程度。公式如下： entropy=\sum _ { x \in X } - p ( x ) \log _ { 2 } p ( x )信息增益，公式如下： I G ( S , A ) = \mathrm { H } ( S ) - \sum _ { t \in T } p ( t ) \mathrm { H } ( t )ID3算法： 遍历所有特征，找到IG最大的特征。 使用该特征进行划分。并删除该特征。 循环1-2，直到所有数据为相同类别，或者达到叶子节点最小数量要求。 C4.5C4.5树是ID3的一个后续，使用信息增益率作为分裂标准，可以使用连续值作为特征(对于一个连续特征，取所有数据中出现的值，排序之后，取相邻的两个值的均值作为划分点)。采用后剪枝防止过拟合。 缺点：只能针对分类问题。 CARTCART树支持回归问题，对于分类问题使用基尼系数的增益作为分裂标准。对于回归问题使用MSE作为分裂标准。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XGBoost结果分析]]></title>
    <url>%2F2019%2F01%2F03%2FXGBoost%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[使用eli5进行解释这里参考eli5的文档，使用kaggle泰坦尼克数据集为例，对XGBoost的结果进行解释。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>xgboost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XGBoost使用]]></title>
    <url>%2F2019%2F01%2F03%2FXGBoost%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>xgboost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XGBoost原理]]></title>
    <url>%2F2019%2F01%2F01%2FXGBoost%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[决策树决策树可以分为3类，采用不同的分裂标准，(1)采用信息增益，ID3算法。(2)采用信息增益率，c4.5算法。(3)采用基尼指数CART树。 CART树 XGBoost使用CART(Classification And Regression Tree, 分类回归树)作为基分类器。 基尼指数公式： \operatorname { Gini } ( D ) = \sum _ { k = 1 } ^ { K } p _ { k } \left( 1 - p _ { k } \right)基尼指数增益公式： \operatorname { Gini } ( D , A ) = \frac { \left| D _ { 1 } \right| } { | D | } \operatorname { Gini } \left( D _ { 1 } \right) + \frac { \left| D _ { 2 } \right| } { | D | } \operatorname { Gini } \left( D _ { 2 } \right)GBDTXGBoost目标函数，目标函数如下 \operatorname { Obj } ( t ) = \sum _ { i = 1 } ^ { n } L \left( y _ { i } , \hat { y } ^ { t - 1 } + f _ { t } \left( x _ { i } \right) \right) + \Omega \left( f _ { t } \right) + \text { constant }其中constant是一个常数，$\Omega \left( f _ { t } \right)$为正则项。 正则项如下： \Omega \left( f _ { t } \right) = \gamma T + \frac { 1 } { 2 } \lambda \sum _ { i = 1 } ^ { T } w _ { j } ^ { 2 }优化目标函数是为了计算每个叶子节点的权重。 附录 XGBoost比GradientBoost好的原因： 增加正则项，树越复杂，惩罚越大，防止over-fitting, 而GB没有惩罚项。 在迭代更新的时候，XGBoost采用了二阶导数(海瑟矩阵)，而GB只用了一阶导数(梯度)。 XGBoost采用了并行计算，块处理，稀疏矩阵等技术。 节点分裂方式不同，gbdt使用gini指数，XGBoost是经过优化推导后的。 XGBoost进化历程： 决策树 -&gt; 对样本重抽样，多棵树平均-&gt; Tree Bagging -&gt; 对特征进行随机挑选 -&gt; 随机森林 -&gt; 对随机森林中的树进行加权平均，而非简单平均 -&gt; Boosting -&gt; 对Boosting中的树进行正则化 -&gt; XGBoost 对比模型：MART, DART(DART:Dropouts meet Multiple Additive Regression Trees), LightGBM, CatBoost]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>xgboost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F01%2F01%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
