<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[掏粪男孩]]></title>
    <url>%2F2019%2F02%2F18%2F%E6%8E%8F%E7%B2%AA%E7%94%B7%E5%AD%A9%2F</url>
    <content type="text"><![CDATA[这篇博客记录tensorflow一些知识。 tensorflow读取机制在算法识别图片的时候，要先把图片读到内存中，然后再输入GPU进行读取。假设每张图片读取要0.1s，计算要0.9s，那么每一秒中GPU就有0.1s无事可做。为了充分利用GPU，使用两个线程，一个线程先将数据读取到内存队列，另一个线程直接从内存队列中读取数据进行计算。在tensorflow中，在内存队列前又增加了一个文件名队列。 为什么要增加文件名队列呢？平时实验中一个epoch就是把数据从头到尾的跑一边，实验一般有多个epoch，有几个epoch就把文件名在文件队列中复制几遍即可，这样就方便了内存队列，不用因为多个epoch而做出改变。 1234567891011121314151617181920212223# 导入tensorflowimport tensorflow as tf # 新建一个Sessionwith tf.Session() as sess: # 我们要读三幅图片A.jpg, B.jpg, C.jpg filename = ['A.jpg', 'B.jpg', 'C.jpg'] # string_input_producer会产生一个文件名队列 filename_queue = tf.train.string_input_producer(filename, shuffle=False, num_epochs=5) # reader从文件名队列中读数据。对应的方法是reader.read reader = tf.WholeFileReader() key, value = reader.read(filename_queue) # tf.train.string_input_producer定义了一个epoch变量，要对它进行初始化 tf.local_variables_initializer().run() # 使用start_queue_runners之后，才会开始填充队列 threads = tf.train.start_queue_runners(sess=sess) i = 0 while True: i += 1 # 获取图片数据并保存 image_data = sess.run(value) with open('read/test_%d.jpg' % i, 'wb') as f: f.write(image_data)]]></content>
  </entry>
  <entry>
    <title><![CDATA[CNN]]></title>
    <url>%2F2019%2F02%2F15%2FCNN%2F</url>
    <content type="text"><![CDATA[卷积神经网络卷积层尺寸的计算原理 输入矩阵格式：四个维度，依次为：样本数、图像高度、图像宽度、图像通道数 输出矩阵格式：与输出矩阵的维度顺序和含义相同，但是后三个维度（图像高度、图像宽度、图像通道数）的尺寸发生变化。 权重矩阵（卷积核）格式：同样是四个维度，但维度的含义与上面两者都不同，为：卷积核高度、卷积核宽度、输入通道数、输出通道数（卷积核个数） 关系 卷集核的通道数由输入矩阵决定。 输出矩阵的通道数由卷集核的数量决定。 输出矩阵的高度和宽度计算如下：h_{out}=(h_{in} - h_{k} + 2*padding)/stride + 1w_{out}=(w_{in} - w_{k} + 2*padding)/stride + 1 tensorflow中卷积层的实现12345678def conv_layer(x, out_channel, k_size, stride, padding): in_channel = x.shape[3].value w = tf.Variable(tf.truncated_normal([k_size, k_size, in_channel, out_channel], mean=0, stddev=stddev)) b = tf.Variable(tf.zeros(out_channel)) y = tf.nn.conv2d(x, filter=w, strides=[1, stride, stride, 1], padding=padding) y = tf.nn.bias_add(y, b) y = tf.nn.relu(y) return x 关于padding有两种方式，“SAME”和“VALID”，表示如下： height_{valid} = \lceil{\frac{h_{in}-h_k+1}{stride}}\rceilheight_{same} = \lceil{\frac{h_{in}}{stride}}\rceil对比普通神经网络的优点VGG19残差网络从理论上，增加网络的层数，使网络更加复杂能够提取更加复杂的特征。但是通过实验发现，当网络层数增加时，网络准确度出现饱和甚至下降。56层的网络效果比20层的网络差。这是因为深层网络存在梯度消失或者梯度爆炸的问题。使得深度网络难以训练。 残差学习从极端情况考虑，在浅层的网络上加入恒等变换层，可以得到和浅层网络相同的效果。因此可以在这个基础上加入残差，降低学习复杂度。从直观上看残差学习需要学习的内容少，因为残差一般会比较小，学习难度小点。 残差单元可以表示为如下的形式f_{i+1}=\delta(h(f_i) + F(f_i, W_l))，其中$\delta$表示激活函数，在ResNet中是relu，$h(x)$表示恒等变换，$F(x)$是残差函数，表示学习到的残差。一般每个残差单元有多层的结构，表示如下：f_{L}=f_l + \sum_{i=l}^{L}{F(f_i, W_l)}利用链式规则进行求梯度如下： \frac{\partial loss}{\partial f_l}=\frac{\partial loss}{\partial f_L}\frac{\partial f_L}{\partial f_l}=\frac{\partial loss}{\partial x_L}(1+\frac{\partial\sum_{i=l}^{L}{F(x_i, W)}}{\partial x_l})其中1代表了短路机制，可以让梯度更容的反向传播。 ResNet网络结构ResNet参考了VGG19网络，在其基础上进行修改加入了残差单元。变化主要体现在，ResNet直接使用stride=2的卷积做下采样，并且用global average pool代替了最后的全连接层。 DenseNetBatchNorm梯度消失和梯度爆炸关于深度神经网络出现梯度消失和梯度爆炸的问题，要从BP算法上看。一个简单的神经网络绘制成下图所示。 神经网络公式表示如下： f_i = \sigma(f_{i-1}*W_i+b_i)其中$f_i, W_i, b_i$分别代表第$i$层的输出，系数和偏置项。使用$f_0$表示输入x。$\sigma$代表sigmod函数。 BP算法基于梯度下降策略，以目标的负梯度方向对参数进行调整。参数更新策略为$w=w+\Delta w$，其中$\Delta w=-\alpha \frac{\partial loss}{\partial w}$ 以$b_2$的反向梯度传播为例，\Delta b_2=\frac{\partial loss}{\partial b_2}=\frac{\partial loss}{\partial f_4}\frac{\partial f_4}{\partial f_3}\frac{\partial f_3}{\partial f_2}\frac{\partial f_2}{\partial b_2} 其中每一层的反向梯度都是对激活函数的求导之后乘上$W_i$。 梯度消失问题如果每一层的激活函数求导的值都小于1，那么经过很多层之后，浅层的梯度就接近于0。此时浅层得不到有效的训练，因此增加了层数未必效果就好。一般w初始化在0-1之间，所以总的乘积小于1。 另外从直觉上理解，loss对于w的梯度，代表w有微小改变时，loss的变化量。在经过sigmod函数的时候，尽管w变化很大，sigmod的输出也会变化很小，特别是经过多层sigmod函数之后，变化就更小了。 Nielsen在《Neural Networks and Deep Learning》中的实验结果如下图，可以看出越浅的层训练的速度越慢，越深的层训练速度越快。第一层的速度比第四层慢了100倍。 第二，不同的激活函数对于梯度消失的影响不同。例如sigmod函数，如下图所示，$\sigma(x)=\frac{1}{1+e^{-x}}$的导数为$\sigma^{\prime}=\sigma*(1-\sigma)$。而sigmod函数的取值范围是(0, 1)，因此sigmod函数导数的取值最大为0.25。当神经网络层数增加之后，梯度消失的很快。 同理，tanh作为激活函数$tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$，如下图所示，比sigmod好一些，但是同样小于1。$tanh^{\prime}(x)=1-tanh^2(x)$ 梯度爆炸问题如果每一层激活函数求导的值都大于1，那么经过很多层之后，浅层的梯度就是个很大的数字，浅层也同样的不到有效的训练。 解决方法 预训练加微调，在深度信念网络(DBN)中，每层依次训练，训练完成之后对整个网络进行微调，得到全局最优，目前用的不是很多了。 针对梯度爆炸问题，可以使用梯度裁剪的方法，将梯度限制在某个范围内。或者对权重进行l1和l2正则化。 使用relu，leakrelu，elu等激活函数。$relu(x)=max(x, 0)$，如下图所示。在$x&gt;0$的部分导数等于1，因此在深度神经网络中不容出现梯度消失和梯度爆炸的问题。relu的优点有：(1)解决了梯度消失、爆炸的问题；(2)计算方便，计算速度快；(3)加速了网络的训练。缺点有：(1)由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决）；(2)输出不是以0为中心的。 使用batchnorm的方式解决。在反向传播中，$\frac{\partial f_2}{\partial x} = \frac{\partial f_2}{\partial f_1}w$，反向传播中有w存在，w的大小影响了梯度消失和梯度爆炸。BN的存在通过对每一层的输出规范为均值和方差一致的方法，消除了w带来的放大缩小的影响进，而解决梯度消失和爆炸的问题，或者可以理解为BN将输出从饱和区拉倒了非饱和区。 使用残差网络解决 LSTM]]></content>
  </entry>
  <entry>
    <title><![CDATA[序列模型]]></title>
    <url>%2F2019%2F02%2F01%2F%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[RNN其他网络只能接受固定长度的输入，并且输出固定长度的向量。 RNN有以下几种模式 第一张图片就是非序列的神经网络，例如CNN。 第二张图片输入不是序列，输出是一个序列，例如：看图说话。 第三张图片输入是一个序列，输出是一个值，例如：文本分类。 第四张图片输入是一个序列，输出也是一个序列，但不是同步的。例如：机器翻译。 第五张图片输入是一个序列，输出也是一个序列，并且是同步的。例如：视频分类，每一帧一个分类。 BPTT LSTM基础LSTM和GRUBI-LSTMConvLSTMAttentionAttention is all you need. TransformerBert 应用 推荐：xiangnan he. 轨迹预测 NLP：语音，语言模型，翻译，图片标题]]></content>
  </entry>
  <entry>
    <title><![CDATA[《编程可读代码的艺术》读书笔记]]></title>
    <url>%2F2019%2F01%2F29%2F%E3%80%8A%E7%BC%96%E7%A8%8B%E5%8F%AF%E8%AF%BB%E4%BB%A3%E7%A0%81%E7%9A%84%E8%89%BA%E6%9C%AF%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[最近在学习如何重构代码，在此记录一些重构代码的技术和方法。 代码应当易于理解，代码的写法应当使别人理解它所需的时间最小化。 表面层次的改进选择更好的名字，写好的注释，整洁的代码格式 把信息装进名字里 选择专业的词，可以使用同义词词典。 单词 更多选择 send deliver, dispatch, announce, distribute, route find search, extract, locate, recover start launch, create, begin, open make create, set up, build, generate, compose, add, new 避免使用tmp，retval这样泛泛的名字。除非有好的理由，除了用在变量值交换的时候，其他时候最好用变量的具体含义起名字。在多层嵌套的for循环中，使用前缀比直接使用i, j, k能有效的防止出错。例如： 123456789101112131415161718192021for(int i=0; i &lt; clubs.size(); ++i) &#123; for(int j=0; j &lt; clubs[i].members.size(); ++j) &#123; for(int k=0; k &lt; users.size(); ++k) &#123; if(clubs[i].members[k] == users[j]) &#123; ... &#125; &#125; &#125;&#125;// 上面的代码index多了容易出错，把member和users的index弄混了。// 使用前缀的写法如下for(int ci=0; ci &lt; clubs.size(); ++ci) &#123; for(int mi=0; mi &lt; clubs[ci].members.size(); ++mi) &#123; for(int ui=0; ui &lt; users.size(); ++ui) &#123; if(clubs[ci].members[mi] == users[ui]) &#123; ... &#125; &#125; &#125;&#125; 用具体的名字代替抽象的名字例如：—run_locally，目的是使用本地的数据库，应该改为—use_local_database 为名字附带更多信息有关变量的重要信息可以加入变量名中。例如string id存储的是16进制的ID，那么可以改名为hex_id。 带单位的值如果变量是一个度量的话，最好把名字带上单位。例如变量尾部追加ms代表毫秒。 变量的长度作用域小的标识符可以使用短的名字，作用域大的标识符名字要包含更多的信息。 首字母缩略词和缩写如果缩写不能让新成员看懂，那么就不要用缩写。常见的有string -&gt; str。 丢掉没有用的单词例如ConverToString可以改为ToString。 使用不会误解的名字有些名字会被理解成其他的含义，例如：BIG_LIMIT=10，可以理解成&lt;10，也可以理解成&lt;=10。容易带来歧义。 使用min，max来表示包含的极限。在要限制的东西前面加上max或者min。 使用first，last来表示包含的范围。 使用begin，end来表示包含/排除的范围。 对于bool值，前面加上is，has，can，should这样的词可以把bool值变得更加明确。不要使用反义名字，名字表示肯定意思。例如：不要使用bool disable_ssl=false，使用bool use_ssl=false。 使用与含义相匹配的名字。例如：对于get函数，期望的复杂度是O(1)，如果复杂的是O(n)则会影响使用。 审美三条基本原则： 使用一致的布局，让读者很快就习惯这种风格。 让相似的代码看上去相似。 把相关的代码行分组，形成代码块。 重新安排换行来保持一致和紧凑 12345678910111213141516171819202122public class PerformanceTest &#123; public static final TcpConnectionSimulator wifi = new TcpConnectionSimulator( 500, /* Kbps */ 80, /* millisecs latency */ 200, /* jitter */ 1 /* packet loss % */ ); public static final TcpConnectionSimulator t3_fiber = new TcpConnectionSimulator( 45000, /* Kbps */ 80, /* millisecs latency */ 200, /* jitter */ 10 /* packet loss % */ ); public static final TcpConnectionSimulator cell = new TcpConnectionSimulator( 100, /* Kbps */ 80, /* millisecs latency */ 1 /* packet loss % */ );&#125; 上面的代码把注释复制了三遍，占用了过多的纵向空间。通过下面的写法可以写的更加紧凑。 123456789public class PerformanceTest &#123; // TcpConnectionSimulator(throughput, latency, jitter, packet_loss) // [Kbps] [ms] [ms] [percent] public static final TcpConnectionSimulator wifi = new TcpConnectionSimulator(500, 80, 200, 1); public static final TcpConnectionSimulator t3_fiber = new TcpConnectionSimulator(45000, 10, 0, 0); public static final TcpConnectionSimulator cell = new TcpConnectionSimulator(100, 400, 250, 5); 把声明按块组织起来，例如：构造函数，析构函数放在一起，request，post方法放在一起。 把代码分成段落，用空行按逻辑分隔，并写法一致。 注释 好的代码自带解释性，起好的函数名能够自己解释清楚。 记录思想，例如：注释教会读者一些东西，防止做无谓的优化。解释代码为什么写的不整洁。 加入TODO，FIXME等标签来记录代码的瑕疵。 给常量加注释，解释常量可以告诉读者为什么这么用。例如：0.72是经过尝试的最好的参数。 站在读者的角度，把读者想要提问的地方加上注释。 什么地方不需要注释：能够代码本身推断的事实；用来粉饰烂代码的拐杖。 应该记录的想法包括：对于代码为什么写成这样而不是那样的理由；代码中的缺陷；常量背后的故事，为什么是这个值。 站在读者的立场上思考：预料到读者的问题；为意料之外的代码加上注释；在文件/类级别上使用“全局观”注释来解释所有部分是如何一起工作的；用注释来总结代码块，是的这不要迷失在细节上。 写出言简意赅的注释]]></content>
  </entry>
  <entry>
    <title><![CDATA[数据可视化]]></title>
    <url>%2F2019%2F01%2F26%2F%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[可视化工具可视化工具有如下几种，seaborn可以看作对matplotlib的高级封装；Plotly可以制作可交互的图片；Bokeh同样也是制作可交互的图片；ggplot是R包ggplot2的python版本，绘制静态图片；Graph visualization with NetworkX用于绘制图数据结构的包。 类别变量可视化类别变量可视化使用seaborn的catplot函数，可以绘制类别的散点图(stripplot, swarmplot)，类别分布图(boxplot, violinplot, boxenplot)，类别估计图(pointplot, barplot, countplot) stripplotcatplot kind参数的默认值就是stripplot12345678910""" total_bill tip sex smoker day time size0 16.99 1.01 Female No Sun Dinner 21 10.34 1.66 Male No Sun Dinner 32 21.01 3.50 Male No Sun Dinner 33 23.68 3.31 Male No Sun Dinner 24 24.59 3.61 Female No Sun Dinner 4"""tips = sns.load_dataset("tips")sns.catplot(x="day", y="total_bill", data=tips); 使用jitter参数控制每个类别的宽度。 1sns.catplot(x="day", y="total_bill", jitter=False, data=tips); swarmplotswarmplot把数据按树形展开，防止重叠，并体现出了数据的分布。类似于琴图。1sns.catplot(x="day", y="total_bill", hue="sex", kind="swarm", data=tips); 使用order参数，传入一个list，可以控制类别的先后顺序。 boxplot关于箱型图，箱型图包括中位数，上四分位数，下四分位数，上限，下限，异常点。分位距IQR=Q3-Q1，异常值是大于Q3+1.5IQR或者小于Q1-1.5IQR范围内的值。1sns.catplot(x="day", y="total_bill", kind="box", data=tips); boxenplot相对于boxplot，boxenplot展示了更多的信息。123diamonds = sns.load_dataset("diamonds")sns.catplot(x="color", y="price", kind="boxen", data=diamonds.sort_values("color")); violinplot琴图结合了箱型图和核密度估计12sns.catplot(x="total_bill", y="day", hue="time", kind="violin", data=tips); barplot12titanic = sns.load_dataset("titanic")sns.catplot(x="sex", y="survived", hue="class", kind="bar", data=titanic); pointplotpointplot 在barplot的基础上比较了不同类之间的大小 1sns.catplot(x="sex", y="survived", hue="class", kind="point", data=titanic);]]></content>
  </entry>
  <entry>
    <title><![CDATA[数据预处理]]></title>
    <url>%2F2019%2F01%2F20%2Ffeature-preprocess%2F</url>
    <content type="text"><![CDATA[数据预处理流程 判断数据类型，判断每一列数据的类型，float，category，boolean，text。 删除只有一个值的特征。 删除相同的特征，有些特征虽然值不一样，但只是用了不同的代号而已。 根据特征的类型产生新的特征。例如，对于连续的特征，每个月的销量的累加，对于神经网络，可以自动产生这种特征，但是对于树模型，无法处理这种特征。对于类别的特征，参考下面类别特征。 可以加入近邻数据的特征的平均值作为新的特征，但是定义近邻距离时，首先要确定不同的特征组(可以根据特征的均值分类)，之后给定不同特征组不同的系数，然后求出距离。 Numeric Feature对于线性模型需要Feature scaling，不同的scale会对梯度下降有很大的影响，scale大的feature的微小变动对结果影响很大。 Scaling MinMaxScaler：不改变分布 StandardScaler Outliers使用可视化的方法，发现outliers。 Feature clipping: 使用99%的分位数作为最大的。 Rank：使用排序代替原始值，如果无法使用可视化发现outliers 使数据变紧凑 np.log(1+x), np.sqrt(1+x) Categorical and ordinal featurescategorical feature没有序，例如：性别。ordinal feature有序，例如：仓位，年级。 对于Tree，使用Label encoding或者Frequency encoding。对于其他模型，使用One-hot necoding。不同的categorical feature或者ordinal feature之间可以做交互。例如：性别和仓位，变成1male, 2male, 1female, 2female。这样的做法对于线性模型有很大帮助。 Datetime and coordinates features对于Datetime feature可以转换为，day of week, week of year, season of year；可以转换为距离某一个特定日期的距离，例如距离周末的距离；是否是节假日； 对于Coordinates feature可以转化为距离某一个点，可以是train set中的，也可以是额外的(地标，城市中心)的距离，转换为格子内的统计信息。 缺失值缺失值的类型有：NA，空字符串，-1，很大的数字，-99999，999，99。通过可视化的方式，可以找出缺失值。 可以使用mean或者median值填充，也可以使用拟合的方式填充。增加一列标记是否缺失的二值变量。 不要在产生新的feature之前对缺失值做填充，否则会影响产生的feature。 文本特征文本预处理，在提取文本特征之前要对文本做预处理，预处理包括：(1)将文本变为小写，同样的单词大写和小写的意义是相同的。(2)提取词干，将名词，形容词转换为相同的词干。(3)Lemmatization，将名词，形容词都转换为名词形式，过去式转换为现在式。(4)去除停词。这一步可以使用sklearn.feature_extraction.text.CountVectorizer，其中参数max_df指去除出现次数多少次以上的单词。 BOW提取特征，每个单词作为数据中的一个维度，使用sklearn.feature_extraction.text.CountVectorizer来完成这个工作，其中Ngram_range指定使用多少个gram, analyzer指定以单词为单位还是字母为单位。由于不同文本的长度不同，所以单词出现的频率也不同。需要使用tf-idf来衡量每个单词的重要性。 tf-idf原理，在一份给定的文件里，词频（term frequency，tf）指的是某一个给定的词语在该文件中出现的频率。这个数字是对词数（term count）的归一化，以防止它偏向长的文件。tf可以表示为： \mathrm { tf } _ { \mathrm { i } , \mathrm { j } } = \frac { n _ { i , j } } { \sum _ { k } n _ { k , j } }逆向文件频率（inverse document frequency，idf）是一个词语普遍重要性的度量。某一特定词语的idf，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取以10为底的对数得到： \operatorname { idf } _ { \mathrm { i } } = \lg \frac { | D | } { \left| \left\{ j : t _ { i } \in d _ { j } \right\} \right| }其中$|D|$：语料库中的文件总数$ | {j : t i \in d_j}| $包含词语$t_i$的文件数目（即 $n{i,j} \neq 0$的文件数目）如果词语不在数据中，就导致分母为零，因此一般情况下使用 $1 + \left| \left{ j : t { i } \in d { j } \right} \right|$然后 \operatorname { tfidf } _ { \mathrm { i } , \mathrm { j } } = \mathrm { tf } _ { \mathrm { i } , \mathrm { j } } \times \mathrm { idf } _ { \mathrm { i } }例子：假如一篇文件的总词语数是100个，而词语“母牛”出现了3次，那么“母牛”一词在该文件中的词频就是3/100=0.03。而计算文件频率（IDF）的方法是以文件集的文件总数，除以出现“母牛”一词的文件数。所以，如果“母牛”一词在1,000份文件出现过，而文件总数是10,000,000份的话，其逆向文件频率就是lg（10,000,000 / 1,000）=4。最后的tf-idf的分数为0.03 * 4=0.12。 如果只针对单词提取特征，疑问句和陈述句可能得到相同的结果。例如：’This is the first document.’ 和 ‘Is this the first document?’。为了防止词组顺序颠倒，除了提取一元模型 1-grams（个别词）之外，我们还可以提取 2-grams 的单词。 Word2Vec， 数据可视化可视化可以帮助我们发现有用的feature，并且使用他们产生新的feature。通常来说，没有特定的一套方法能够对所有的数据可视化。需要不断的输出数据，绘制数据，发现有用的数据之后进行进一步的探索。 数据可视化有以下方法： Histograms，绘制直方图可以看出数据的分布。但是直方图有时候可能会带来错觉。如下图，左边的直方图看来，所有数据都是0。对数据取log(1+x)之后可以发现，数据有很多不同的值。尖峰的原因可能是很多缺失值被用平均值或者中位数填充了。 标准化在机器学习算法的目标函数(例如SVM的RBF内核或线性模型的l1和l2正则化)，许多学习算法中目标函数的基础都是假设所有的特征都是零均值并且具有同一阶数上的方差。如果某个特征的方差比其他特征大几个数量级，那么它就会在学习算法中占据主导位置，导致学习器并不能像我们说期望的那样，从其他特征中学习。 标准化能够加速梯度下降，如下图：左边蓝色的图片，由于$x_2$的范围比较大，因此$w_2$的微小变化损失函数就有很大的变化。反之$w_1$要变化很多损失函数才有明显变化。从梯度下降的公式来说，$w_1$的变化取决于$x_1$，因此梯度很小，那么需要很久才能下降到最低点。右边的图片做了标准化，能够很快的下降到最低点。 其他变换QuantileTransformer把数据变成对应的分为数，例如原本处于0.25分位数的数字是6.8，变化之后就是0.25。这个变换是一个非线性变换。 归一化，是 缩放单个样本以具有单位范数的过程。 数据清洗清洗常量的数据，常量的数据对于分类没有帮助，需要去除掉。1234567# `dropna = False` makes nunique treat NaNs as a distinct valuefeats_counts = train.nunique(dropna = False)feats_counts.sort_values()[:10]constant_features = feats_counts.loc[feats_counts==1].index.tolist()print (constant_features)traintest.drop(constant_features,axis = 1,inplace=True) 清洗重复的数据，对于不同的列，可能使用符号不同，但是本质是相同的。1234567891011121314train_enc = pd.DataFrame(index = train.index)for col in tqdm_notebook(traintest.columns): train_enc[col] = train[col].factorize()[0]dup_cols = &#123;&#125;for i, c1 in enumerate(tqdm_notebook(train_enc.columns)): for c2 in train_enc.columns[i + 1:]: if c2 not in dup_cols and np.all(train_enc[c1] == train_enc[c2]): dup_cols[c2] = c1import cPickle as picklepickle.dump(dup_cols, open('dup_cols.p', 'w'), protocol=pickle.HIGHEST_PROTOCOL)traintest.drop(dup_cols.keys(), axis = 1,inplace=True)]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>数据处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[张江895孵化器访问]]></title>
    <url>%2F2019%2F01%2F16%2F%E5%BC%A0%E6%B1%9F895%E5%AD%B5%E5%8C%96%E5%99%A8%E8%AE%BF%E9%97%AE%2F</url>
    <content type="text"><![CDATA[由于寒假社会生活实践的原因，今天我拜访了位于张江高科的895医疗健康孵化器。这家公司成立于2013年，于2017年12月正式开始运营孵化器，大部分的公司处在A轮之前，产品处于生产出来之前。目前有8家孵化企业，追求小而精的团队。其中设计AI的有3家，大部分是医疗器械和智慧养老。这家孵化器属于民营公司，主要通过已有的创业经验来帮助创业团队，帮助走过创业的初期。对比学校的孵化器所做的产品更加深入，对比政府的孵化器，能够在应用和技术的细节上为创业团队提供帮助，而不只是政策引导的帮助(政府的孵化器相当于是老教授，民营孵化器相当于是高年级学长)。管理团队来自各个创业团队的主要成员。895医疗健康孵化器对于挑选入住孵化器的团队主要看3个方面，(1)技术创新性，临床需求；(2)公司的成熟度是否和孵化器匹配；(3)和已经入住的团队是否匹配。 这是我第一次了解什么是孵化器，孵化器有什么作用，以及人工智能在医疗领域的应用现状。 一般来讲，创业开始都是有了一个不错的idea或者是有了一项创新的技术。但是创立一个公司却不单单是这些。创始人要考虑出了技术之外的很多方面，例如：财务，法务，客户等等问题。一个没有经验的创业者，会被这些问题阻碍。因此针对出了技术之外的繁杂问题，孵化器主要从以下三个方面提供了帮助，(1)软硬件设施；(2)行业资源；(3)生态圈。 软硬件设施。首先是孵化器为创业团队提供了办公场所，但这是最不重要的一点。如果只是办公场所，孵化器和二房东就没有区别了。例如：一个团队做出来算法的创新，需要做一款APP来推向市场。传统的方式是自己聘用做APP的人，或者外包出去。前者由于开发不是重点，如果没有足够的资金，那么难以保证质量，而且一时间也难以组建一个合适的开发团队。后者外包的开发的团队一般对于创业的领域了解程度不够，在对接需求上会花费大量的经历，也会影响产品的质量(这一点我深有体会，在2018年11月上线一个产品时，由于开发团队没有金融的基础，每次新来一个业务人员，我需要把整个背景讲一遍，处理问题的细节时也经常出错)。孵化器里面的企业类型相同或者相似，直接配以开发团队，轮流为各个创业团队提供开发服务，既为开发团队解决了不必要的问题，又深入了业务，减少了大量的错误，节省了成本。除此之外，一个产品开发出来，后期的跟踪很重要。但是创业团队不一定有专业的专门的人负责这件事情。例如：监测老人身体状况的手环，孵化器配备的人员会定期回访手环使用情况。检测到老人摔倒，到底是真的摔倒，还是老人在做什么事情。医疗数据有数据量小，但是要求准确性高的特点。通过后期跟踪，能够及时的得到数据反馈，调整模型。第三，平台能够让产品尽快实践，得到实际的反馈，产品操作是否便捷，培训是否简单。这些决定了最终是否有人买账。以上的软硬件设施或者团队，在创业初期很难建设完全。但是这些模块缺一不可，孵化器就成了一个很好的支持。 行业资源。医疗器械的成熟慢，一个产品要5-8年的周期，过程中要设计方方面面。因此把一个想法或者一个算法变成产品，行业资源是不可缺少的。在医疗行业，国家有非常严格的监管。对于每一个产品如何验证其安全性，如何设计临床实验，如何保证生产中的质量。这些大坑足以把绝大部分创业者卡死，毕竟医疗行业产品要保证最差的时候也要有很高的准确率。一次错误也不能犯。孵化器在这些方面有丰富的资源，能够及时的为创业团队提供这些服务。 生态圈。孵化器中同时存在相同类型的多家公司，是一个很好的生态圈，不同公司之间往往会擦出不可思议的火花。例如：一个做医疗穿戴的团队和智慧社区养老的团队合作，同时解决了两个团队的问题。在一个大的平台上能够拓宽思路和产品的销路。 总体来说，医疗行业和互联网行业还是有很大的区别。医疗行业的成熟速度慢，很保守，产品要有严格的安全性。尽管目前有很多论文，但是产品的下限还难以保证，因此人工智能在医疗领域的应用还有很大的发展空间。 待续。。。]]></content>
      <categories>
        <category>生活游记</category>
      </categories>
      <tags>
        <tag>生活游记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GANs]]></title>
    <url>%2F2019%2F01%2F14%2FGANs%2F</url>
    <content type="text"><![CDATA[对抗生成网络笔记这篇文章用来记录各种GAN的原理和应用。 EBGAN(ENERGY-BASED GENERATIVE ADVERSARIAL NETWORKS)作者：Junbo Zhao, Michael Mathieu and Yann LeCun发表：ICLR 2017]]></content>
  </entry>
  <entry>
    <title><![CDATA[集成学习]]></title>
    <url>%2F2019%2F01%2F05%2F%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[随机森林]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>Ensemble Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树]]></title>
    <url>%2F2019%2F01%2F05%2F%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[决策树优点 容易解释，可以可视化。 对数据预处理的要求低，不需要做数据归一化，允许数据有缺失。 可以处理连续型变量和离散型变量。 决策树的缺点 容易过拟合，可以通过剪枝，设置每个节点最小样本数量，树的最大深度来避免。 容易收到干扰，某些数据的微小变化可能带来树的巨大变化。其实就是方差的问题，减小方差可以通过ensemble的方式缓解。 无法保证找到最优的树，找到最优的树是一个NPC问题。因此，每个节点分裂时，使用贪心算法。只能得到局部最优解。可以通过ensemble的方式缓解，训练每棵树时对数据和特征做采样。 There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems. 对于数据不平衡问题，树可能有很大的偏差，需要先解决数据不平衡问题。 时间复杂度分析使用技巧 数据量和特征数量要匹配，数据很少，但是特征很多很容易过拟合。 提前使用PCA，ICA或者Feature Selection降维可以让树学的更好。 事先解决数据不平衡问题。例如通过采样方式，或者通过normalizing the sum of the sample weights for each class的方式实现。 不同类型的树ID3ID3树每个节点有多个分支，根据信息增益来作为分裂标准。在树生成之后使用剪枝来防止过拟合。 缺点：只能使用离散特征，只能针对分类问题。 信息熵，表示数据的不确定程度。公式如下： entropy=\sum _ { x \in X } - p ( x ) \log _ { 2 } p ( x )信息增益，公式如下： I G ( S , A ) = \mathrm { H } ( S ) - \sum _ { t \in T } p ( t ) \mathrm { H } ( t )ID3算法： 遍历所有特征，找到IG最大的特征。 使用该特征进行划分。并删除该特征。 循环1-2，直到所有数据为相同类别，或者达到叶子节点最小数量要求。 C4.5C4.5树是ID3的一个后续，使用信息增益率作为分裂标准，可以使用连续值作为特征(对于一个连续特征，取所有数据中出现的值，排序之后，取相邻的两个值的均值作为划分点)。采用后剪枝防止过拟合。 缺点：只能针对分类问题。 CARTCART树支持回归问题，对于分类问题使用基尼系数的增益作为分裂标准。对于回归问题使用MSE作为分裂标准。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XGBoost结果分析]]></title>
    <url>%2F2019%2F01%2F03%2FXGBoost%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[使用eli5进行解释这里参考eli5的文档，使用kaggle泰坦尼克数据集为例，对XGBoost的结果进行解释。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>xgboost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XGBoost使用]]></title>
    <url>%2F2019%2F01%2F03%2FXGBoost%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>xgboost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XGBoost原理]]></title>
    <url>%2F2019%2F01%2F01%2FXGBoost%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[决策树决策树可以分为3类，采用不同的分裂标准，(1)采用信息增益，ID3算法。(2)采用信息增益率，c4.5算法。(3)采用基尼指数CART树。 CART树 XGBoost使用CART(Classification And Regression Tree, 分类回归树)作为基分类器。 基尼指数公式： \operatorname { Gini } ( D ) = \sum _ { k = 1 } ^ { K } p _ { k } \left( 1 - p _ { k } \right)基尼指数增益公式： \operatorname { Gini } ( D , A ) = \frac { \left| D _ { 1 } \right| } { | D | } \operatorname { Gini } \left( D _ { 1 } \right) + \frac { \left| D _ { 2 } \right| } { | D | } \operatorname { Gini } \left( D _ { 2 } \right)GBDTXGBoost目标函数，目标函数如下 \operatorname { Obj } ( t ) = \sum _ { i = 1 } ^ { n } L \left( y _ { i } , \hat { y } ^ { t - 1 } + f _ { t } \left( x _ { i } \right) \right) + \Omega \left( f _ { t } \right) + \text { constant }其中constant是一个常数，$\Omega \left( f _ { t } \right)$为正则项。 正则项如下： \Omega \left( f _ { t } \right) = \gamma T + \frac { 1 } { 2 } \lambda \sum _ { i = 1 } ^ { T } w _ { j } ^ { 2 }优化目标函数是为了计算每个叶子节点的权重。 附录 XGBoost比GradientBoost好的原因： 增加正则项，树越复杂，惩罚越大，防止over-fitting, 而GB没有惩罚项。 在迭代更新的时候，XGBoost采用了二阶导数(海瑟矩阵)，而GB只用了一阶导数(梯度)。 XGBoost采用了并行计算，块处理，稀疏矩阵等技术。 节点分裂方式不同，gbdt使用gini指数，XGBoost是经过优化推导后的。 XGBoost进化历程： 决策树 -&gt; 对样本重抽样，多棵树平均-&gt; Tree Bagging -&gt; 对特征进行随机挑选 -&gt; 随机森林 -&gt; 对随机森林中的树进行加权平均，而非简单平均 -&gt; Boosting -&gt; 对Boosting中的树进行正则化 -&gt; XGBoost 对比模型：MART, DART(DART:Dropouts meet Multiple Additive Regression Trees), LightGBM, CatBoost]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>xgboost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F01%2F01%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
